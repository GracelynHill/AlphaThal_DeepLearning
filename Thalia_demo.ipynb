{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Thalia_demo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This file is a demonstration of how the deep learning models were trained in practice for the poster presentation."
      ],
      "metadata": {
        "id": "DsMipBf_Vd3G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We begin by mounting the drive and reading the set of features. The program to convert a BAM file to this set of features will be included in the final github repo."
      ],
      "metadata": {
        "id": "Npm01msMDMrL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znWUfLOpq22a"
      },
      "outputs": [],
      "source": [
        "#mounting drive and importing data\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "df = pd.read_table('/content/drive/My Drive/FeatureSet2.withheader.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using pandas, we shuffle the data and filter out the Genotype labels into a second dataset."
      ],
      "metadata": {
        "id": "EgbRMsYZDuU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#seperating targets from training data\n",
        "def shuffle_sep(df):\n",
        "  df = df.sample(frac=1)\n",
        "  df_targy = df.filter(regex='Genotype')\n",
        "  df_targy['Genotype'] = pd.factorize(df['Genotype'])[0]\n",
        "  df = df.drop('Genotype', 1)\n",
        "  return df, df_targy\n",
        "def shuffle_sep_G(df):\n",
        "  df = df.sample(frac=1)\n",
        "  df_targy = df.filter(regex='Genotype')\n",
        "  Genotypes = df_targy.Genotype.unique()\n",
        "  df_targy['Genotype'] = pd.factorize(df['Genotype'])[0]\n",
        "  df = df.drop('Genotype', 1)\n",
        "  return df, df_targy, Genotypes\n",
        "df, df_targy, Genotypes = shuffle_sep_G(df)\n",
        "def just_sep(df):\n",
        "  df_targy = df.filter(regex='Genotype')\n",
        "  df = df.drop('Genotype', 1)\n",
        "  return df, df_targy\n"
      ],
      "metadata": {
        "id": "Q3mRGa1btpt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we transfer the pandas dataframes to numpy arrays, which Keras takes as input."
      ],
      "metadata": {
        "id": "Gom5E7E7D9r9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#initializing numpy arrays\n",
        "def sets_numpy(df, df_targy):\n",
        "  train = df.to_numpy()\n",
        "  targy = df_targy.to_numpy()\n",
        "  return train, targy\n"
      ],
      "metadata": {
        "id": "91-sxMNavtI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-hot encoding is then employed on the labels, which is necessary for a classification problem like this one."
      ],
      "metadata": {
        "id": "XGoXtZS9EF0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#vectorization\n",
        "from keras.utils.np_utils import to_categorical\n",
        "def vectorize_and_holdout(train, targy):\n",
        "  v_train = train\n",
        "  v_targy = to_categorical(targy)\n",
        "  return v_train, v_targy\n"
      ],
      "metadata": {
        "id": "KaROjS9bwlZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function allows us to define hyperparameters for k-fold validation."
      ],
      "metadata": {
        "id": "-NhJX4NTEWQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#k-fold validation training\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import regularizers\n",
        "def run_thalia(input, trainset, targyset, k, num_epochs, numlayers, weights, kernel, dropout):\n",
        "  all_scores = []\n",
        "  all_losses = []\n",
        "  all_tlosses = []\n",
        "  num_val_samples = len(trainset) // k\n",
        "  all_scores = []\n",
        "  for i in range(k):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Dense(weights, kernel_regularizer=regularizers.l2(0.001),\n",
        "                        activation='relu', input_shape=(input,)))\n",
        "    for j in range(numlayers):\n",
        "      if kernel == True:\n",
        "        model.add(layers.Dense(weights, kernel_regularizer=regularizers.l2(0.001),\n",
        "                        activation='relu'))\n",
        "      if kernel != True:\n",
        "        model.add(layers.Dense(weights, activation='relu'))\n",
        "      if dropout == True:\n",
        "        model.add(layers.Dropout(0.25))\n",
        "    model.add(layers.Dense(6, activation='softmax'))\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    print('processing fold #', i + 1)\n",
        "    val_data = trainset[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "    val_targy = targyset[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "\n",
        "    partial_v_train = np.concatenate(\n",
        "        [trainset[:i * num_val_samples],\n",
        "        trainset[(i + 1) * num_val_samples:]],\n",
        "        axis = 0)\n",
        "    partial_v_targy = np.concatenate(\n",
        "        [targyset[:i * num_val_samples],\n",
        "        targyset[(i + 1) * num_val_samples:]],\n",
        "        axis = 0)\n",
        "    history = model.fit(partial_v_train, partial_v_targy, epochs= num_epochs, batch_size=512, verbose = 0, validation_data = (val_data, val_targy))\n",
        "    val_loss, val_mae = model.evaluate(val_data, val_targy, verbose = 0)\n",
        "    all_scores.append(val_mae)\n",
        "    print(all_scores)\n",
        "    print(val_loss)\n",
        "  return history, model"
      ],
      "metadata": {
        "id": "BGCA5j3U0ITE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function allows us to train best possible versions of models, but the validation accuracy & loss outputs are more variable because unlike k-fold validation, it is not taking a mean of several model runs."
      ],
      "metadata": {
        "id": "CayfjkrRemHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_thalia_all(input, trainset, targyset, num_epochs, numlayers, weights, kernel, dropout):\n",
        "  all_scores = []\n",
        "  all_losses = []\n",
        "  all_tlosses = []\n",
        "  num_val_samples = len(trainset)\n",
        "  all_scores = []\n",
        "  model = models.Sequential()\n",
        "  if kernel==True:\n",
        "    model.add(layers.Dense(weights, kernel_regularizer=regularizers.l2(0.001),\n",
        "                      activation='relu', input_shape=(input,)))\n",
        "  else:\n",
        "      model.add(layers.Dense(weights,\n",
        "                      activation='relu', input_shape=(input,)))\n",
        "  for j in range(numlayers):\n",
        "    if kernel == True:\n",
        "      model.add(layers.Dense(weights, kernel_regularizer=regularizers.l2(0.001),\n",
        "                      activation='relu'))\n",
        "    if kernel != True:\n",
        "      model.add(layers.Dense(weights, activation='relu'))\n",
        "    if dropout == True:\n",
        "      model.add(layers.Dropout(0.25))\n",
        "  model.add(layers.Dense(6, activation='softmax'))\n",
        "  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  print('processing')\n",
        "  history = model.fit(trainset, targyset, epochs= num_epochs, batch_size=512, verbose = 0, validation_split= 0.01)\n",
        "  return history, model"
      ],
      "metadata": {
        "id": "aVH0qZqKMuH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code was used to train the model that tested against the real data."
      ],
      "metadata": {
        "id": "BWaXf8mceyVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cdf2 = pd.read_table('/content/drive/My Drive/DataSet2.5880samples.cvg30.insert440.stdev100.justcvg.hg19.txt')\n",
        "cdf2, cdf2_targy, Genotypes = shuffle_sep_G(cdf2)\n",
        "print(cdf2_targy[0:20])\n",
        "ctrain2, ctargy2 = sets_numpy(cdf2, cdf2_targy)\n",
        "v_train, v_targy = vectorize_and_holdout(ctrain2, ctargy2)\n",
        "endhistory, endmodel = run_thalia_all(430, v_train, v_targy, 100, 0, 2048, False, True)"
      ],
      "metadata": {
        "id": "N3V-fGCdZC_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code predicts genotypes for real data and writes the output to files, for comparison purposes."
      ],
      "metadata": {
        "id": "ic4s5G11e4fF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.engine.data_adapter import pack_x_y_sample_weight\n",
        "print(endhistory.history['val_accuracy'])\n",
        "realdata = pd.read_table('/content/drive/My Drive/RealSamples.unnormalized.nogenotypes.hg19.test.norm.maskedgenos.final.txt')\n",
        "realdata, trash = just_sep(realdata)\n",
        "realdata = realdata.to_numpy()\n",
        "print(realdata)\n",
        "predictions = endmodel.predict(realdata)\n",
        "print(predictions)\n",
        "finalpredicts = []\n",
        "def genetest():\n",
        "  for i in predictions:\n",
        "    bestval = 0\n",
        "    for index, value in enumerate(i):\n",
        "      if i[index] > bestval:\n",
        "        bestval = i[index]\n",
        "        bestindex = index\n",
        "    finalpredicts.append(Genotypes[bestindex])\n",
        "with open('predictions_prob.txt', 'w') as f:\n",
        "  for G in Genotypes:\n",
        "    f.write(G + \"\\t\")\n",
        "  for i in predictions:\n",
        "    for index, value in enumerate(i):\n",
        "      f.write(str(value) + \"\\t\")\n",
        "    f.write('\\n')\n",
        "genetest()\n",
        "with open('predictions_best.txt', 'w') as f:\n",
        "  for p in finalpredicts:\n",
        "    f.write(p + '\\n')"
      ],
      "metadata": {
        "id": "-0kVxNiWapNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next code cells allow us to visualize a comparison between accuracies aligned to different reference genomes, or with different levels of coverage."
      ],
      "metadata": {
        "id": "zUyKB4yafCvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df4 = pd.read_table('/content/drive/My Drive/DataSet2.cvg10.insert440.stdev100.hg19.txt')\n",
        "df3 = pd.read_table('/content/drive/My Drive/DataSet2.cvg20.insert440.stdev100.hg19.txt')\n",
        "df2 = pd.read_table('/content/drive/My Drive/FeatureSet2.bundle2.withheader.txt')\n",
        "df2, df2_targy = shuffle_sep(df2)\n",
        "df3, df3_targy = shuffle_sep(df3)\n",
        "df4, df4_targy = shuffle_sep(df4)\n",
        "train2, targy2 = sets_numpy(df2, df2_targy)\n",
        "train3, targy3 = sets_numpy(df3, df3_targy)\n",
        "train4, targy4 = sets_numpy(df4, df4_targy)\n",
        "v2_train, v2_targy = vectorize_and_holdout(train2, targy2)\n",
        "v3_train, v3_targy = vectorize_and_holdout(train3, targy3)\n",
        "v4_train, v4_targy = vectorize_and_holdout(train4, targy4)\n",
        "history, model = run_thalia_all(1305, v_train, v_targy, 12, 5, 1024, False, False)\n",
        "history2, model2 = run_thalia_all(1305, v2_train, v2_targy, 12, 7, 128, False, False)\n",
        "history3, model3 = run_thalia_all(1290, v3_train, v3_targy, 12, 2, 512, False, False)\n",
        "history4, model4 = run_thalia_all(1290, v4_train, v4_targy, 12, 8, 256, False, False)\n"
      ],
      "metadata": {
        "id": "MKeHrdKnyrLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cdf2 = pd.read_table('/content/drive/My Drive/DataSet2.5880samples.cvg30.insert440.stdev100.hg19.txt')\n",
        "cdf2, cdf2_targy, Genotypes = shuffle_sep_G(cdf2)\n",
        "ctrain2, ctargy2 = sets_numpy(cdf2, cdf2_targy)\n",
        "v_train, v_targy = vectorize_and_holdout(ctrain2, ctargy2)\n",
        "\n",
        "l1, model = run_thalia_all(1290, v_train, v_targy, 12, 0, 16, True, False)\n",
        "l2, model = run_thalia_all(1290, v_train, v_targy, 12, 0, 32, True, False)\n",
        "l3, model = run_thalia_all(1290, v_train, v_targy, 12, 0, 64, True, False)\n",
        "l4, model = run_thalia_all(1290, v_train, v_targy, 12, 0, 128, True, False)\n",
        "l5, model = run_thalia_all(1290, v_train, v_targy, 12, 0, 256, True, False)\n",
        "l6, model = run_thalia_all(1290, v_train, v_targy, 12, 0, 512, True, False)\n",
        "l7, model = run_thalia_all(1290, v_train, v_targy, 12, 0, 1024, True, False)\n",
        "l8, model = run_thalia_all(1290, v_train, v_targy, 12, 0, 2048, True, False)\n",
        "\n"
      ],
      "metadata": {
        "id": "9S6KFTqv7HQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tcdf3 = pd.read_table('/content/drive/My Drive/DataSet2.5880samples.cvg30.insert440.stdev100.chm13.txt')\n",
        "tcdf3, tcdf3_targy = shuffle_sep(tcdf3)\n",
        "tctrain3, tctargy3 = sets_numpy(tcdf3, tcdf3_targy)\n",
        "tcv3_train, tcv3_targy = vectorize_and_holdout(tctrain3, tctargy3)\n",
        "tl1, model = run_thalia_all(1306, tcv3_train, tcv3_targy, 12, 0, 16, True, False)\n",
        "tl2, model = run_thalia_all(1306, tcv3_train, tcv3_targy, 12, 0, 32, True, False)\n",
        "tl3, model = run_thalia_all(1306, tcv3_train, tcv3_targy, 12, 0, 64, True, False)\n",
        "tl4, model = run_thalia_all(1306, tcv3_train, tcv3_targy, 12, 0, 128, True, False)\n",
        "tl5, model = run_thalia_all(1306, tcv3_train, tcv3_targy, 12, 0, 256, True, False)\n",
        "tl6, model = run_thalia_all(1306, tcv3_train, tcv3_targy, 12, 0, 512, True, False)\n",
        "tl7, model = run_thalia_all(1306, tcv3_train, tcv3_targy, 12, 0, 1024, True, False)\n"
      ],
      "metadata": {
        "id": "w0wwTKtboiKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_vall1 = l1.history['val_loss']\n",
        "loss_vall2 = l2.history['val_loss']\n",
        "loss_vall3 = l3.history['val_loss']\n",
        "loss_vall4 = l4.history['val_loss']\n",
        "loss_vall5 = l5.history['val_loss']\n",
        "loss_vall6 = l6.history['val_loss']\n",
        "loss_vall7 = l7.history['val_loss']\n",
        "tloss_vall1 = tl1.history['val_loss']\n",
        "tloss_vall2 = tl2.history['val_loss']\n",
        "tloss_vall3 = tl3.history['val_loss']\n",
        "tloss_vall4 = tl4.history['val_loss']\n",
        "tloss_vall5 = tl5.history['val_loss']\n",
        "tloss_vall6 = tl6.history['val_loss']\n",
        "tloss_vall7 = tl7.history['val_loss']\n",
        "\n",
        "\n",
        "loss_valls = [loss_vall1, loss_vall2, loss_vall3, loss_vall4, loss_vall5, loss_vall6, loss_vall7]\n",
        "last_losses = []\n",
        "for i in loss_valls:\n",
        "  last_loss = i[-1]\n",
        "  last_losses.append(last_loss)\n",
        "\n",
        "tloss_valls = [tloss_vall1, tloss_vall2, tloss_vall3, tloss_vall4, tloss_vall5, tloss_vall6, tloss_vall7]\n",
        "tlast_losses = []\n",
        "for i in tloss_valls:\n",
        "  tlast_loss = i[-1]\n",
        "  tlast_losses.append(tlast_loss)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "nlayers = range(4, 11)\n",
        "plt.rcParams['figure.figsize'] = [10, 10]\n",
        "plt.plot(nlayers, last_losses, 'r', label= 'hg19 Validation Loss')\n",
        "plt.plot(nlayers, tlast_losses, 'b', label='T2T Validation Loss')\n",
        "\n",
        "\n",
        "plt.title('Validation Loss')\n",
        "\n",
        "plt.xlabel('2^n Weights')\n",
        "\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NDB9Jwky7VB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ll1, model = run_thalia_all(1305, v_train, v_targy, 12, 1, 1024, False, False)\n",
        "ll2, model = run_thalia_all(1305, v_train, v_targy, 12, 2, 1024, False, False)\n",
        "ll3, model = run_thalia_all(1305, v_train, v_targy, 12, 3, 1024, False, False)\n",
        "ll4, model = run_thalia_all(1305, v_train, v_targy, 12, 4, 1024, False, False)\n",
        "ll5, model = run_thalia_all(1305, v_train, v_targy, 12, 5, 1024, False, False)\n",
        "ll6, model = run_thalia_all(1305, v_train, v_targy, 12, 6, 1024, False, False)\n",
        "ll7, model = run_thalia_all(1305, v_train, v_targy, 12, 7, 1024, False, False)\n",
        "ll8, model = run_thalia_all(1305, v_train, v_targy, 12, 8, 1024, False, False)"
      ],
      "metadata": {
        "id": "2szbJO0RN6MG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss_train = history.history['accuracy']\n",
        "\n",
        "loss_val = history.history['val_accuracy']\n",
        "\n",
        "loss_train2 = history2.history['accuracy']\n",
        "\n",
        "loss_val2 = history2.history['val_accuracy']\n",
        "\n",
        "loss_train3 = history3.history['accuracy']\n",
        "\n",
        "loss_val3 = history3.history['val_accuracy']\n",
        "\n",
        "loss_train4 = history4.history['accuracy']\n",
        "\n",
        "loss_val4 = history4.history['val_accuracy']\n",
        "\n",
        "\n",
        "epochs = range(1,13)\n",
        "plt.rcParams['figure.figsize'] = [10, 10]\n",
        "\n",
        "\n",
        "plt.plot(epochs, loss_train, 'pink', label='T2T_10x Training accuracy')\n",
        "\n",
        "plt.plot(epochs, loss_val, 'brown', label='T2T_10x validation accuracy')\n",
        "\n",
        "plt.plot(epochs, loss_train2, 'g', label='T2T_20x Training accuracy')\n",
        "\n",
        "plt.plot(epochs, loss_val2, 'b', label='T2T_20x validation accuracy')\n",
        "\n",
        "plt.plot(epochs, loss_train3, 'r', label='hg19_20x Training accuracy')\n",
        "\n",
        "plt.plot(epochs, loss_val3, 'y', label='hg19_20x validation accuracy')\n",
        "\n",
        "plt.plot(epochs, loss_train4, 'c', label='hg19_10x Training accuracy')\n",
        "\n",
        "plt.plot(epochs, loss_val4, 'm', label='hg19_10x validation accuracy')\n",
        "\n",
        "\n",
        "plt.title('Training and Validation accuracy')\n",
        "\n",
        "plt.xlabel('Epochs')\n",
        "\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dURvuEYk0EgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss_train = history.history['loss']\n",
        "\n",
        "loss_val = history.history['val_loss']\n",
        "\n",
        "loss_train2 = history2.history['loss']\n",
        "\n",
        "loss_val2 = history2.history['val_loss']\n",
        "\n",
        "loss_train3 = history3.history['loss']\n",
        "\n",
        "loss_val3 = history3.history['val_loss']\n",
        "\n",
        "loss_train4 = history4.history['loss']\n",
        "\n",
        "loss_val4 = history4.history['val_loss']\n",
        "\n",
        "\n",
        "epochs = range(5,13)\n",
        "plt.rcParams['figure.figsize'] = [10, 10]\n",
        "\n",
        "\n",
        "plt.plot(epochs, loss_train[4:], 'pink', label='T2T_10x Training loss')\n",
        "\n",
        "plt.plot(epochs, loss_val[4:], 'brown', label='T2T_10x validation loss')\n",
        "\n",
        "plt.plot(epochs, loss_train2[4:], 'g', label='T2T_20x Training loss')\n",
        "\n",
        "plt.plot(epochs, loss_val2[4:], 'b', label='T2T_20x validation loss')\n",
        "\n",
        "plt.plot(epochs, loss_train3[4:], 'r', label='hg19_10x Training loss')\n",
        "\n",
        "plt.plot(epochs, loss_val3[4:], 'y', label='hg19_10x validation loss')\n",
        "\n",
        "plt.plot(epochs, loss_train4[4:], 'c', label='hg19_20x Training loss')\n",
        "\n",
        "plt.plot(epochs, loss_val4[4:], 'm', label='hg19_20x validation loss')\n",
        "\n",
        "\n",
        "plt.title('Training and Validation loss')\n",
        "\n",
        "plt.xlabel('Epochs')\n",
        "\n",
        "plt.ylabel('loss')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dGhlbDbdyHmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cdf4 = pd.read_table('/content/drive/My Drive/DataSet2.11760samples.cvg10.insert440.stdev100.hg19.txt')\n",
        "cdf3 = pd.read_table('/content/drive/My Drive/DataSet2.11760samples.cvg20.insert440.stdev100.hg19.txt')\n",
        "cdf2 = pd.read_table('/content/drive/My Drive/DataSet2.11760samples.cvg30.insert440.stdev100.hg19.txt')\n",
        "cdf1 = pd.read_table('/content/drive/My Drive/DataSet2.11760samples.cvg40.insert440.stdev100.hg19.txt')\n",
        "cdf1, cdf1_targy = shuffle_sep(cdf1)\n",
        "cdf2, cdf2_targy = shuffle_sep(cdf2)\n",
        "cdf3, cdf3_targy = shuffle_sep(cdf3)\n",
        "cdf4, cdf4_targy = shuffle_sep(cdf4)\n",
        "ctrain1, ctargy1 = sets_numpy(cdf1, cdf1_targy)\n",
        "ctrain2, ctargy2 = sets_numpy(cdf2, cdf2_targy)\n",
        "ctrain3, ctargy3 = sets_numpy(cdf3, cdf3_targy)\n",
        "ctrain4, ctargy4 = sets_numpy(cdf4, cdf4_targy)\n",
        "cv1_train, cv1_targy = vectorize_and_holdout(ctrain1, ctargy1)\n",
        "cv2_train, cv2_targy = vectorize_and_holdout(ctrain2, ctargy2)\n",
        "cv3_train, cv3_targy = vectorize_and_holdout(ctrain3, ctargy3)\n",
        "cv4_train, cv4_targy = vectorize_and_holdout(ctrain4, ctargy4)\n",
        "chistory, cmodel = run_thalia_all(1290, cv1_train, cv1_targy, 10, 7, 1024, False, False)\n",
        "chistory2, cmodel2 = run_thalia_all(1290, cv2_train, cv2_targy, 10, 4, 1024, False, False)\n",
        "chistory3, cmodel3 = run_thalia_all(1290, cv3_train, cv3_targy, 10, 8, 256, False, False)\n",
        "chistory4, cmodel4 = run_thalia_all(1290, cv4_train, cv4_targy, 10, 2, 512, False, False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "f3nen2qIYXVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss_train = chistory.history['loss']\n",
        "\n",
        "loss_val = chistory.history['val_loss']\n",
        "\n",
        "loss_train2 = chistory2.history['loss']\n",
        "\n",
        "loss_val2 = chistory2.history['val_loss']\n",
        "\n",
        "loss_train3 = chistory3.history['loss']\n",
        "\n",
        "loss_val3 = chistory3.history['val_loss']\n",
        "\n",
        "loss_train4 = chistory4.history['loss']\n",
        "\n",
        "loss_val4 = chistory4.history['val_loss']\n",
        "\n",
        "\n",
        "epochs = range(1,11)\n",
        "plt.rcParams['figure.figsize'] = [10, 10]\n",
        "\n",
        "\n",
        "plt.plot(epochs, loss_train, 'pink', label='40x Training loss')\n",
        "\n",
        "plt.plot(epochs, loss_val, 'brown', label='40x validation loss')\n",
        "\n",
        "plt.plot(epochs, loss_train2, 'g', label='30x Training loss')\n",
        "\n",
        "plt.plot(epochs, loss_val2, 'b', label='30x validation loss')\n",
        "\n",
        "plt.plot(epochs, loss_train3, 'r', label='20x Training loss')\n",
        "\n",
        "plt.plot(epochs, loss_val3, 'y', label='20x validation loss')\n",
        "\n",
        "plt.plot(epochs, loss_train4, 'c', label='10x Training loss')\n",
        "\n",
        "plt.plot(epochs, loss_val4, 'm', label='10x validation loss')\n",
        "\n",
        "\n",
        "plt.title('Training and Validation loss')\n",
        "\n",
        "plt.xlabel('Epochs')\n",
        "\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CsULpmQ1ZmhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_valls = [loss_val4, loss_val3, loss_val2, loss_val]\n",
        "last_losses = []\n",
        "for i in loss_valls:\n",
        "  last_loss = i[-1]\n",
        "  last_losses.append(last_loss)\n",
        "import matplotlib.pyplot as plt\n",
        "print(last_losses)\n",
        "nfolds = range(1, 5)\n",
        "plt.rcParams['figure.figsize'] = [10, 10]\n",
        "plt.plot(nfolds, last_losses, 'r', label='Validation Loss')\n",
        "plt.title('Validation Loss')\n",
        "\n",
        "plt.xlabel('Coverage (divided by 10)')\n",
        "\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0yo8iMYJDZ-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cdf4 = pd.read_table('/content/drive/My Drive/DataSet2.5880samples.cvg40.insert440.stdev100.justcvg.hg19.txt')\n",
        "cdf3 = pd.read_table('/content/drive/My Drive/DataSet2.5880samples.cvg30.insert440.stdev100.justcvg.hg19.txt')\n",
        "cdf2 = pd.read_table('/content/drive/My Drive/DataSet2.5880samples.cvg20.insert440.stdev100.justcvg.hg19.txt')\n",
        "cdf1 = pd.read_table('/content/drive/My Drive/DataSet2.5880samples.cvg10.insert440.stdev100.justcvg.hg19.txt')\n",
        "cdf1, cdf1_targy = shuffle_sep(cdf1)\n",
        "cdf2, cdf2_targy = shuffle_sep(cdf2)\n",
        "cdf3, cdf3_targy = shuffle_sep(cdf3)\n",
        "cdf4, cdf4_targy = shuffle_sep(cdf4)\n",
        "ctrain1, ctargy1 = sets_numpy(cdf1, cdf1_targy)\n",
        "ctrain2, ctargy2 = sets_numpy(cdf2, cdf2_targy)\n",
        "ctrain3, ctargy3 = sets_numpy(cdf3, cdf3_targy)\n",
        "ctrain4, ctargy4 = sets_numpy(cdf4, cdf4_targy)\n",
        "cv1_train, cv1_targy = vectorize_and_holdout(ctrain1, ctargy1)\n",
        "cv2_train, cv2_targy = vectorize_and_holdout(ctrain2, ctargy2)\n",
        "cv3_train, cv3_targy = vectorize_and_holdout(ctrain3, ctargy3)\n",
        "cv4_train, cv4_targy = vectorize_and_holdout(ctrain4, ctargy4)\n",
        "chistory, cmodel = run_thalia_all(1290, cv1_train, cv1_targy, 100, 0, 2048, True, True)\n",
        "chistory2, cmodel2 = run_thalia_all(1290, cv2_train, cv2_targy, 100, 0, 2048, False, True)\n",
        "chistory3, cmodel3 = run_thalia_all(1290, cv3_train, cv3_targy, 100, 2, 1024, False, True)\n",
        "chistory4, cmodel4 = run_thalia_all(1290, cv4_train, cv4_targy, 100, 2, 2048, False, True)\n",
        "tcdf4 = pd.read_table('/content/drive/My Drive/DataSet2.5880samples.cvg40.insert440.stdev100.justcvg.chm13.txt')\n",
        "tcdf3 = pd.read_table('/content/drive/My Drive/DataSet2.5880samples.cvg30.insert440.stdev100.justcvg.chm13.txt')\n",
        "tcdf2 = pd.read_table('/content/drive/My Drive/DataSet2.5880samples.cvg20.insert440.stdev100.justcvg.chm13.txt')\n",
        "tcdf1 = pd.read_table('/content/drive/My Drive/DataSet2.5880samples.cvg10.insert440.stdev100.justcvg.chm13.txt')\n",
        "tcdf1, tcdf1_targy = shuffle_sep(tcdf1)\n",
        "tcdf2, tcdf2_targy = shuffle_sep(tcdf2)\n",
        "tcdf3, tcdf3_targy = shuffle_sep(tcdf3)\n",
        "tcdf4, tcdf4_targy = shuffle_sep(tcdf4)\n",
        "tctrain1, tctargy1 = sets_numpy(tcdf1, tcdf1_targy)\n",
        "tctrain2, tctargy2 = sets_numpy(tcdf2, tcdf2_targy)\n",
        "tctrain3, tctargy3 = sets_numpy(tcdf3, tcdf3_targy)\n",
        "tctrain4, tctargy4 = sets_numpy(tcdf4, tcdf4_targy)\n",
        "tcv1_train, tcv1_targy = vectorize_and_holdout(tctrain1, tctargy1)\n",
        "tcv2_train, tcv2_targy = vectorize_and_holdout(tctrain2, tctargy2)\n",
        "tcv3_train, tcv3_targy = vectorize_and_holdout(tctrain3, tctargy3)\n",
        "tcv4_train, tcv4_targy = vectorize_and_holdout(tctrain4, tctargy4)\n",
        "tchistory, tcmodel = run_thalia_all(1306, tcv1_train, tcv1_targy, 20, 4, 512, True, False)\n",
        "tchistory2, tcmodel2 = run_thalia_all(1306, tcv2_train, tcv2_targy, 20, 1, 1024, True, True)\n",
        "tchistory3, tcmodel3 = run_thalia_all(1306, tcv3_train, tcv3_targy, 20, 0, 1024, False, True)\n",
        "tchistory4, tcmodel4 = run_thalia_all(1306, tcv4_train, tcv4_targy, 20, 0, 256, True, False)"
      ],
      "metadata": {
        "id": "qEcKIx22Kt1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_val = chistory.history['val_accuracy']\n",
        "\n",
        "loss_val2 = chistory2.history['val_accuracy']\n",
        "\n",
        "loss_val3 = chistory3.history['val_accuracy']\n",
        "\n",
        "loss_val4 = chistory4.history['val_accuracy']\n",
        "\n",
        "\n",
        "tloss_val = tchistory.history['val_accuracy']\n",
        "\n",
        "tloss_val2 = tchistory2.history['val_accuracy']\n",
        "\n",
        "tloss_val3 = tchistory3.history['val_accuracy']\n",
        "\n",
        "tloss_val4 = tchistory4.history['val_accuracy']\n",
        "\n",
        "\n",
        "loss_valls = [loss_val4, loss_val3, loss_val2, loss_val]\n",
        "last_losses = []\n",
        "for i in loss_valls:\n",
        "  last_loss = i[-1]\n",
        "  last_losses.append(last_loss)\n",
        "tloss_valls = [tloss_val, tloss_val2, tloss_val3, tloss_val4]\n",
        "tlast_losses = []\n",
        "for i in tloss_valls:\n",
        "  tlast_loss = i[-1]\n",
        "  tlast_losses.append(tlast_loss)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "print(last_losses)\n",
        "print(tlast_losses)\n",
        "nfolds = range(1, 5)\n",
        "fig,ax = plt.subplots()\n",
        "fig.canvas.draw()\n",
        "labels = ['0', '10x', '15x', '20x', '25x', '30x', '35x', '40x']\n",
        "plt.rcParams['figure.figsize'] = [10, 10]\n",
        "plt.plot(nfolds, last_losses, 'r', label='hg19 Validation Accuracy')\n",
        "plt.plot(nfolds, tlast_losses, 'b', label='T2T Validation Accuracy')\n",
        "ax.set_xticklabels(labels)\n",
        "plt.title('Validation Accuracy')\n",
        "\n",
        "plt.xlabel('Coverage')\n",
        "\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Y9TTYsK7ocm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss_train = chistory.history['accuracy']\n",
        "\n",
        "loss_val = chistory.history['val_accuracy']\n",
        "\n",
        "loss_train2 = chistory2.history['accuracy']\n",
        "\n",
        "loss_val2 = chistory2.history['val_accuracy']\n",
        "\n",
        "loss_train3 = chistory3.history['accuracy']\n",
        "\n",
        "loss_val3 = chistory3.history['val_accuracy']\n",
        "\n",
        "loss_train4 = chistory4.history['accuracy']\n",
        "\n",
        "loss_val4 = chistory4.history['val_accuracy']\n",
        "\n",
        "\n",
        "epochs = range(1,11)\n",
        "plt.rcParams['figure.figsize'] = [10, 10]\n",
        "\n",
        "\n",
        "plt.plot(epochs, loss_train, 'pink', label='40x Training accuracy')\n",
        "\n",
        "plt.plot(epochs, loss_val, 'brown', label='40x validation accuracy')\n",
        "\n",
        "plt.plot(epochs, loss_train2, 'g', label='30x Training accuracy')\n",
        "\n",
        "plt.plot(epochs, loss_val2, 'b', label='30x validation accuracy')\n",
        "\n",
        "plt.plot(epochs, loss_train3, 'r', label='20x Training accuracy')\n",
        "\n",
        "plt.plot(epochs, loss_val3, 'y', label='20x validation accuracy')\n",
        "\n",
        "plt.plot(epochs, loss_train4, 'c', label='10x Training accuracy')\n",
        "\n",
        "plt.plot(epochs, loss_val4, 'm', label='10x validation accuracy')\n",
        "\n",
        "\n",
        "plt.title('Training and Validation accuracy')\n",
        "\n",
        "plt.xlabel('Epochs')\n",
        "\n",
        "plt.ylabel('accuracy')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "littgOkicD84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code will visualize a given model as a graph. Change the first parameter to change the model visualized."
      ],
      "metadata": {
        "id": "eIrQRnUhfY8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.keras.utils.plot_model(\n",
        "    model,\n",
        "    to_file=\"model.png\",\n",
        "    show_shapes=True,\n",
        "    show_dtype=True,\n",
        "    show_layer_names=False,\n",
        "    rankdir=\"TB\",\n",
        "    expand_nested=False,\n",
        "    dpi=96,\n",
        "    layer_range=None,\n",
        "    show_layer_activations=True,\n",
        ")"
      ],
      "metadata": {
        "id": "OAPVPcj-rOgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code can be used to show the number and type of errors from predictions in a given holdout dataset, but wasn't used for the analysis in the poster."
      ],
      "metadata": {
        "id": "7mD91MKpQxrL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def genetest(numpredict, modelver, t_train, t_targy):\n",
        "  Genotypes = [\"WTYP_WTYP\", \"AL37_WTYP\", \"AL37_AL37\", \"AL42_WTYP\", \"AL42_AL42\", \"AL37_AL42\"]\n",
        "  predictions = modelver.predict(t_train)\n",
        "  counter = 0\n",
        "  errors = 0\n",
        "  for i in predictions[0:numpredict]:\n",
        "    for index, value in enumerate(i):\n",
        "      if i[index] > 0.51:\n",
        "        if Genotypes[int(t_targy[counter])] != Genotypes[index]:\n",
        "          errors += 1\n",
        "          print(Genotypes[index])\n",
        "    counter += 1\n",
        "  print(str(errors) + \" errors\")\n",
        "\n",
        "genetest(100, model3, t3_train, t3_targy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3W1dZXHbV1Vt",
        "outputId": "4ad9a958-a0a0-4d2d-ab10-bfcba0264f0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AL42_WTYP\n",
            "AL42_WTYP\n",
            "2 errors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Y7ZFz_VPGxJe"
      }
    }
  ]
}